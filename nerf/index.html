<!DOCTYPE html>
<html>
<head>
    <title>Final Project: NeRF</title>
    <style>
        body {
            font-family: Times New Roman, Arial, sans-serif;
            margin: 20px;
        }

        <!-- Following code for CSS style is adapted from W3 Schools. https://www.w3schools.com/howto/howto_css_images_side_by_side.asp
        and Stack Overflow
        https://stackoverflow.com/questions/61637178/how-to-center-multiple-figure-elements-in-one-row-->
        * {
      box-sizing: border-box;
    }
    
    .column {
      float: left;
      width: 50%;
      padding: 5px;
    }
    
    
    .row {
      padding-bottom: 50px;
      display: flex;
        text-align: center;
        flex-direction: row;
        justify-content: center;
    }

    img {
        width: 100%;
        height: auto
        
    }
    </style>
</head>

  <body>
    <h1>Welcome to My CS180 Final Project on Neural Radiance Fields!</h1>
    <a href = "../index.html"> My CS180 Personal Project Home Page </a>
    <strong> <br> Name: Zach Turner <br> SID: 3036700008 </strong>
    <h3>Introduction</h3>
      <p>
          In this project, I will build a Neural Radiance Field (NeRF) from scratch using PyTorch. A NeRF is a Machine Learning model that learns a scene--commonly of some object--using a variety of photos taken from various perspectives. Learning from these photos, NeRF allows us to simulate viewing the object from any perspective. For background information, consider reading the <a href = "https://www.matthewtancik.com/nerf">original NeRF paper</a>.
      </p>

      <h3>Part 1</h3>
      <p>
          In part 1, we will start with a simpler objective: learn a single photograph. While this alone is a rather silly task (we already know the photo after all), it is useful warm up for our later implmentation of a full NeRF. 
          <br>
          In this part, the model will learn to map pixel locations {u, v} to RGB values in the photo {r, g, b}. We will train the model for a particular photo. 
      </p>

      <h4>The Network</h4>

      <div class = "row">
        <figure>
            <img src = "media/mlp_img.jpg">
            <figcaption>Network for Part 1</figcaption>
        </figure>
      </div>

      <div class = "row">
        <figure>
            <img src = "media/sinPE.png">
            <figcaption>Sinusodial Positional Encoding</figcaption>
        </figure>
      </div>
      
      <p>
          To accomplish part a, we will use a simple MLP model, as shown above, with sinusodial positional encoding As found in the original paper, NeRF without sinusodial positonal encoding will struggle to learn high frequency image components due to neural networks' preference to learn low frequency functions. Sinusodial positional encoding assists the network to learn high frequency components of the image, since it encode different frequencies in the pixel locations--information the network will use to learn higher frequency image changes. A higher L value will let the network learn higher frequencies and thus create sharper images. It is recommended to use L = 10; I will try using L = 5 and L = 10. We apply sinPE to the x and y coordinates separately. 
          <br><br>
          We will use the pixel locations and rgb values normalized to the interval [0, 1] for training and inference. After we use sinPE on the input coordinates, we will run the information through a fairly basic Multiple Layer Percpetron neural network with 4 linear layers and layer sizes shown above. Note that the final stage uses a sigmoid activation function in order to output rgb values in [0, 1] as desired. 
          <br>
          <br>
          We will train the model on batches of 10000 randomly selected pixels for 1000 to 3000 iterations. I found that the model is close to fully converged after 1000 iterations, with little change occuring between 1000 and 3000 iterations. We will optimize the model using MSE loss and Adam optimization alogorithm. I tried learning rates of 0.01 and 0.001. 
          <br><br>
          In addition to training loss, another relevant metric of model performance is Peak Signal to Noise ratio (PSNR). With images normalized to [0, 1], there exists the relationship PSNR = 10 * log10(1/MSE). 
      </p>

      <h5>Results</h5>
      <p>
          We will begin by training the model on an image of a fox with hyperparameters L = 10 and learning rate = 0.01. 
      </p>

    <div class = "row">
        <figure>
            <img src = "media/fox.jpg">
            <figcaption>Training Image of Fox</figcaption>
        </figure>
        <figure>
            <img src = "media/foxNormal.png">
            <figcaption>Learned Image of Fox</figcaption>
        </figure>
      </div>

      <div class = "row">
        <figure>
            <img src = "media/trainingIms2d.jpg">
            <figcaption>Learned Image of Fox Every 250 Iterations (0 , 250, ... , 1000) </figcaption>
        </figure>
      </div>

      <div class = "row">
        <figure>
            <img src = "media/Unknown-3.png">
            <figcaption>Training Loss</figcaption>
        </figure>
          <figure>
            <img src = "media/Unknown-2.png">
            <figcaption>Training PSNR</figcaption>
        </figure>
      </div>

      <p>
          As we can see, the model does a nice job of learning the fox image. It does contain the imperfections of the grid-like background and lack of detail on the fox's whiskers (which are very high frequency). 
          <br><br>
          Next, let's see what happens with hyperparameters L = 5 and lr = 0.01. (Decreasing L compared to the images above).
      </p>

      <div class = "row">
        <figure>
            <img src = "media/fox.jpg">
            <figcaption>Training Image of Fox</figcaption>
        </figure>
        <figure>
            <img src = "media/foxL5.png">
            <figcaption>Learned Image of Fox (L = 5)</figcaption>
        </figure>
      </div>

      <div class = "row">
        <figure>
            <img src = "media/trainingIms2d5first5.jpg">
            <figcaption>Learned Image of Fox Every 250 Iterations (0 , 250, ... , 1000) </figcaption>
        </figure>
      </div>

      <div class = "row">
        <figure>
            <img src = "media/Unknown-8.png">
            <figcaption>Training Loss (L = 5)</figcaption>
        </figure>
          <figure>
            <img src = "media/Unknown-9.png">
            <figcaption>Training PSNR (L = 5)</figcaption>
        </figure>
      </div>

     <p>
         The images learned with L = 5 are clearly worse than with L = 10. When compared to the images learned with L = 10, the images learned with L = 5 have blurry backgrounds, blurry fur patches, and are missing the fox's whiskers. As all of these issues follow from a lack of high frequency image components, it makes sense that these issues were created by a lower value of L. 

         <br><br>
         For the next hyperparameter combination to try, we know that L = 10 is a better value than L = 5. I have also noticed that the model converges very quickly. Perhaps it could be better to use a lower learning rate. Thus, lets try training the model with L = 10 and learning rate = 0.001 (instead of 0.01). 
     </p>

      <div class = "row">
        <figure>
            <img src = "media/fox.jpg">
            <figcaption>Training Image of Fox</figcaption>
        </figure>
        <figure>
            <img src = "media/Unknown-12.png">
            <figcaption>Learned Image of Fox (L = 5)</figcaption>
        </figure>
      </div>
 
        
      <div class = "row">
        <figure>
            <img src = "media/trainingIms2dlr.jpg">
            <figcaption>Learned Image of Fox Every 250 Iterations (0 , 250, ... , 2000) </figcaption>
        </figure>
      </div>

      <div class = "row">
        <figure>
            <img src = "media/Unknown-10.png">
            <figcaption>Training Loss</figcaption>
        </figure>
          <figure>
            <img src = "media/Unknown-11.png">
            <figcaption>Training PSNR </figcaption>
        </figure>
      </div>

      <p>
          Here we can see that the results are highly similar to when L = 10 and lr = 0.01, except that learning takes more iterations (as would be expected). Thus, I think that the original hyperparameters of L = 10 and lr = 0.01 are the best choice. 
          <br><br>
          Below are all of the images generated under different hyperparameters to compare side by side. 
      </p>

      <div class = "row">
        <figure>
            <img src = "media/fox.jpg">
            <figcaption>Training Image of Fox</figcaption>
        </figure>
        <figure>
            <img src = "media/foxNormal.png">
            <figcaption>Learned Image of Fox (L = 10)</figcaption>
        </figure>
           <figure>
            <img src = "media/foxL5.png">
            <figcaption>Learned Image of Fox (L = 5)</figcaption>
        </figure>
          <figure>
            <img src = "media/Unknown-12.png">
            <figcaption>Learned Image of Fox (lr = 0.001)</figcaption>
        </figure>
          
      </div>

      <h5>Another Image</h5>
      <p>
          Here, I will train this network on a different image, the great reoccurding North Arm Farm image from Pemberton, BC. I will use the hyperparameters L = 10 and lr = 0.01. The results are below.
      </p>

      
      <div class = "row">
        <figure>
            <img src = "media/NAFsmall.jpg">
            <figcaption>Training Image of NAF</figcaption>
        </figure>
        <figure>
            <img src = "media/p1NAFout.jpg">
            <figcaption>Learned Image of NAF</figcaption>
        </figure>
      </div>

      <div class = "row">
          <figure>
              <img src = "media/p1NAFright.jpg">
              <figcaption>Learned Image of NAF Every 250 Iterations (0 , 250, ... , 1000)</figcaption>
          </figure>
      </div>

      
      <div class = "row">
        <figure>
            <img src = "media/trLossNAF.png">
            <figcaption>Training Loss</figcaption>
        </figure>
          <figure>
            <img src = "media/pnsrTrNAF.png">
            <figcaption>Training PSNR </figcaption>
        </figure>
      </div>

      <h2>Part 2: Full NeRF</h2>

      <h4>Overview</h4>

      <p>
          While the previous part yielded a neat result, the next section will be truly epic and has potential for useful applications. In this section, we will fit a full NeRF model based on the Lego scene in the <a href = "https://www.matthewtancik.com/nerf">original NeRF paper</a>. This task will be significantly more complicated, but will allow us to view the Lego from any perspective and ultimately create a 3d rendering of the Lego scene. 
          <br><br>
          By using a dataset with calibrated cameras and known camera locations, we can train the NeRF based on the view of the Lego from various known perspectives. In this dataset, we have 100 training images, 10 validation images, and 60 test images. All images are from unique cameras in unique locations. 
      </p>

      <h5>Part 2.1: Tranformaing Cameras into Rays</h5>

      <p>
          In this section, we develop the mathematical infrastructure necessary to generate rays from pixel and camera locations. Rays are integral to the operation of NeRF. 
          <br><br>
          We begin by implementing a straight-forward batched matrix-vector multiplication function to transform coordinate spaces using various matrices. For example, we often use a camera to world (c2w) matrix, which gives the correspondance between points from in the camera coordinate system to the world coordinate system. The tranform function will append the input coordinates, apply the batch transformation for specified mappings such as c2w matrices, and return the new 3-dimensional coordinates.
          <br><br>
          It is important to know the calibration of the cameras. With that knowledge, we can determine the K matrix using the focal length and image size that, along with a distance parameter s, maps pixel coordinate to camera coordinates. pixel_to_camera applies this batched transformation using specified K and s values. 
          <br><br>
        To complete our goal of determining image rays from pixel coordinates, we need a function to combine these previous function and calculate the resulting rays. Each perspective of an object can be described by an origin ray and a direction ray. The origin ray specifies the location of the camera in 3d world space, and the direction ray describes the direction the pixel is looking in. Based on the pixel locations, K matrix, and c2w matrix, pixel_to_ray will calculate a origin ray and normalized direction ray that describe the perspective of the pixel's view. 
         
          
    
      </p>

      <h5>Sampling</h5>
      <p>
          In this section, we define utility functions and a dataloader that are useful for NeRFs. The dataloader will take in a collection of images, c2w matrices, and K matrix. It is able to randomly sample N rays (with corresponding pixels) for use at each training iteration. We will typically use N = 10000 when training. 
          <br><br>
          Since we want to incorporate depth information into the model, we will sample along the rays along various depths to get information for the NeRF to operate. We use implement this functionality in the the sample_along_rays function. It will return world coordinates at various equally spaced intervals along each ray. During training, we will add a small amount of noise to each interval length to prevent over-fitting. 
      </p>
      
      

  </body>
</html>
